{
 "cells": [
  
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements",
    "The authors would like to thank our colleagues for their support and assistance while writing this paper. There are too many to mention but the following individuals, in particular, contributed a lot of their technical and analytic insights while writing this paper: Gene Grabowski, Timo Kettunen, Deva Kumar, John Stultz, and Jonathan Walker. Our partners at Carnegie Mellon Universityâ€™s Heinz College (Rema Padman, Alexandra Allen, Anzhi Mou, Zhaoyu Qiao, Harvir Singh Virk, Xiaoyu Zhu) showed us innovative, influential and inspiring work with Python and SAS Viya that served as the foundation for some of the topics in this paper. Oscar Heller, Sofia Heller, Alexandria Adams, Benjamin Lannis provided valuable research assistance and insights during early iterations of the conference paper through their data science and storytelling contributions around nursing homes and even opioids. In addition, as always, Kim Andrews for her tireless efforts with all things SAS

A copy of the paper is available here:
https://communities.sas.com/t5/SAS-Global-Forum-Proceedings/Bridging-the-Gap-Between-Legacy-SAS-and-SAS-Viya-at-the-Centers/ta-p/726373"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import swat\n",
    "import os \n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = pd.read_csv('COVID_19_NURSING_HOME_DATASET3F5_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data\n",
    "### Begin be removing columns that we won't be using during our analysis\n",
    "Begin by dropping all the columns that start with z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns that do not have 'z' as first character to keep in dataframe\n",
    "keep_cols=[c for c in nh.columns if c.lower()[:1] != 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe 'nh2' that keeps only the columns we captured above\n",
    "nh2 = nh[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the dimensions of the df, column names, number of null entries, and data types of each column\n",
    "nh2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the target varaible\n",
    "sns.histplot(nh2['Residents_Weekly_Confirmed_COVID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop empty column\n",
    "nh3 = nh2.drop('Total_Nursing_Minutes', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy of df is easy\n",
    "nh4 = nh3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column 'Week_Ending' to date format using pandas 'to_datetime' function\n",
    "nh4.loc[:,\"Week_Ending\"]=pd.to_datetime(nh3['Week_Ending'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Check to see at which date the count data switches from observed values (with decimal digits .000000) to projected values (with more significant decimal digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the last 20 rows of the date and COVID count columns\n",
    "nh4[['Week_Ending', 'Residents_Total_Confirmed_COVID_', 'Residents_Weekly_Confirmed_COVID']].tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After November 1st, projected numbers (not observed values) were placed in the table. We want to get rid of these values so we are only working with real observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-11-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new df 'nh5' that filters 'nh4' to include only rows for which 'Week Ending' is <= Nov 1st\n",
    "nh5 = nh4.loc[nh4['Week_Ending'] <= start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(nh5['Residents_Weekly_Confirmed_COVID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boolean series that indicates whether each nh5 column has datatype object ('== object')\n",
    "categorical_features = nh5.dtypes == object;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset nh5 column names for which above code returned True (where dtype is object)\n",
    "categorical_cols = nh5.columns[categorical_features].to_list() # convert to list, otherwise type would be 'index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts of unique categories in each of the categorical columns by applying pandas function Series.nunique\n",
    "nh5[categorical_cols].apply(pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the same information is provided by columns 'State' and 'Provider_State'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at unique values of column 'State', sorted alphabetically\n",
    "nh5['State'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appears that State and Provider_State are supplying the same information\n",
    "nh5['Provider_State'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that state and provider state are supplying the same information\n",
    "(nh5['Provider_State'] == nh5['State']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are cms_region_location and cms_region providing equivalent information?\n",
    "nh5[['CMS_REGION_LOCATION', 'CMS_REGION']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique combinations of the two variables\n",
    "location_region = nh5['CMS_REGION_LOCATION'] + ' - ' + nh5['CMS_REGION']\n",
    "pd.DataFrame(location_region.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Columns to Drop: CMS_REGION, Summary_Category, Service_Category, Provider_ID, Provider_State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh6 = nh5.drop(['CMS_REGION', 'Summary_Category', 'Service_Category', 'Provider_ID', 'Provider_State'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select numeric cols using pandas .select_dtypes function\n",
    "numeric_cols = nh6.select_dtypes(include = ['int64', 'float64'])\n",
    "numeric_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize continous variarbles\n",
    "numeric_cols.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot resident cases vs. staff cases\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.scatter(nh6['Residents_Weekly_Confirmed_COVID'], nh6['Staff_Weekly_Confirmed_COVID_19'])\n",
    "plt.xlabel('Residents Weekly Confirmed COVID')\n",
    "plt.ylabel('Staff Weekly Confirmed COVID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the target variable over time for all states in the data\n",
    "nh_ts = nh6.pivot(index='Week_Ending', columns='State', values='Residents_Weekly_Confirmed_COVID')\n",
    "nh_ts.plot(figsize = (20, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pairwise correlations as a heatmap\n",
    "plt.figure(figsize=(17,10))\n",
    "sns.heatmap(nh6.corr(), annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect positive correlation between Episode_or_Stay_Count and Distinct_Beneficiaries, which means we should drop one of them. Perfect negative correlation between Percent_Female_Beneficiaries and Percent_Male_Beneficiaries, so one should be dropped. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "### Create target variable\n",
    "The original idea was to predict for number of deaths / cases. I want to investigate if this would really be a good predictor since there is a lag between positive case confirmation and death of the patient. I will make some plots to explore this possible relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate state data together to generate sum of total national weekly resident cases and deaths \n",
    "weekly_cases_deaths = nh6.groupby('Week_Ending')[['Residents_Weekly_Confirmed_COVID', 'Residents_Weekly_COVID_19_Deaths']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot total weekly cases and weekly deaths together in same figure\n",
    "fig, ax1 = plt.subplots() #create figure object and axis object\n",
    "color = 'tab:orange'\n",
    "ax1.plot(weekly_cases_deaths.index, weekly_cases_deaths['Residents_Weekly_Confirmed_COVID'], color=color) #plot (x,y,color) \n",
    "ax1.tick_params(axis='y', labelcolor=color) #tick_params allows for the change of ticks, tick labels, and gridlines\n",
    "ax1.set_xlabel('Week')\n",
    "ax1.set_ylabel('Weekly Confirmed Covid Cases', color=color)\n",
    "\n",
    "ax2 = ax1.twinx() #new axis object with same x as ax1 (Week)\n",
    "color = 'tab:blue'\n",
    "ax2.plot(weekly_cases_deaths.index, weekly_cases_deaths['Residents_Weekly_COVID_19_Deaths'], color=color)\n",
    "ax2.set_ylabel('Weekly Confirmed Covid Deaths', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_title('Weekly Covid Cases vs Weekly Covid Deaths') #add a title\n",
    "\n",
    "fig.tight_layout() #prevantative in case the subplot does not fit into the figure area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine beginning of time series where anomolously high counts occur\n",
    "weekly_cases_deaths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot time series of week ending vs. confirmed cases for NC, UT, TX, and FL\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(nh6.loc[nh6['State'] == 'NC']['Week_Ending'], nh6.loc[nh6['State'] == 'NC']['Residents_Weekly_Confirmed_COVID'], label='NC')\n",
    "ax.plot(nh6.loc[nh6['State'] == 'UT']['Week_Ending'], nh6.loc[nh6['State'] == 'UT']['Residents_Weekly_Confirmed_COVID'], label='UT')\n",
    "ax.plot(nh6.loc[nh6['State'] == 'TX']['Week_Ending'], nh6.loc[nh6['State'] == 'TX']['Residents_Weekly_Confirmed_COVID'], label='TX')\n",
    "ax.plot(nh6.loc[nh6['State'] == 'FL']['Week_Ending'], nh6.loc[nh6['State'] == 'FL']['Residents_Weekly_Confirmed_COVID'], label='FL')\n",
    "ax.set_ylabel('Residents_Weekly_Confirmed_COVID')\n",
    "ax.set_xlabel('Week_Ending')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine data for NC\n",
    "nh6.loc[nh6['State'] == 'NC'][['State', 'Week_Ending', 'Residents_Weekly_Confirmed_COVID']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First four weeks' data seem erroneously high (see graphic of all states). We will ommit this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dt = '2020-05-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to include only weeks greater than or equal to start date\n",
    "nh7 = nh6.loc[nh6['Week_Ending'] >= start_dt ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check results -- examine NC case counts again\n",
    "nh7.loc[nh7['State'] == 'NC'][['State', 'Week_Ending', 'Residents_Weekly_Confirmed_COVID']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create line plot for all states showing weekly confirmed covid cases \n",
    "nh_ts = nh7.pivot(index='Week_Ending', columns='State', values='Residents_Weekly_Confirmed_COVID')\n",
    "nh_ts.plot(figsize = (20, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new dataframe based off of time aggregated values\n",
    "Get a 4 week moving average of all numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the (# rows, # cols) later for a sanity check\n",
    "nh7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh7.Residents_Weekly_Confirmed_COVID[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of what is going on in the following cell:\n",
    "1. Initialize ma_df data frame, and indentify unique states in State column. \n",
    "2. For every column in the nh7 data frame, if the column name starts with 'Resi', 'Staf', or 'Numb' then assign j the column number and initialize an empty list called ma_data. \n",
    "3. For every unique state in the state column, create a df with rows only for that state and reset the index of df since the rows keep their indices from nh7. \n",
    "4. For every row in the df data frame, assign null values to the ma_data list if its one of the first three rows; otherwise, apply a moving average of the current row and the previous three. The moving average is appended to the ma_data list and rounded to two decimals. \n",
    "5. Add the created list as a column to the ma_df data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame of moving averages made from the numeric columns in nh7 df\n",
    "\n",
    "#1\n",
    "ma_df = pd.DataFrame()\n",
    "unique_states = nh7['State'].unique()\n",
    "\n",
    "#2\n",
    "for column in nh7.columns.to_list():\n",
    "    if column[0:4] in ['Resi', 'Staf','Numb']: \n",
    "        j = np.argmax(nh7.columns == column)\n",
    "        ma_data = [] \n",
    "        #3\n",
    "        for state in unique_states: \n",
    "            df = nh7.loc[nh7['State'] == state].copy() \n",
    "            df.reset_index(inplace=True, drop=True)\n",
    "            #4\n",
    "            for i in range(df.shape[0]): \n",
    "                if i in [0, 1, 2]: \n",
    "                    ma_data.append(None) \n",
    "                else:\n",
    "                    ma_data.append(np.round((df.iloc[i,j]+df.iloc[i-1,j]+df.iloc[i-2,j]+df.iloc[i-3,j])/4,2))\n",
    "        #5\n",
    "        ma_df['MA_'+column] = ma_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the dimensions of the ma_df dataframe as a sanity check that the loop worked\n",
    "ma_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine first 5 rows of the ma_df dataframe\n",
    "ma_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Target Variable \n",
    "Target variable ma_infection_beds is a moving average of the current row and the next row (looking into the future)\n",
    "1. create empty list variable ma_infection_beds\n",
    "2. for every unique state in the State column, create a dataframe df and reset the index\n",
    "3. for every row in df, if it is the last row or second-to-last row, then give it a null value. Otherwise, give it a two day moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "ma_infection_beds = []\n",
    "\n",
    "#2\n",
    "for state in nh7['State'].unique():\n",
    "    df = nh7.loc[nh7['State'] == state].copy()\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    #3\n",
    "    for i in range(df.shape[0]):\n",
    "        if i in [df.shape[0]-1, df.shape[0]-2]:\n",
    "            ma_infection_beds.append(None)\n",
    "        else:\n",
    "            ma_infection_beds.append((df.iloc[i+2,2]/df.iloc[i+2,6] * 100 + df.iloc[i+1,2]/df.iloc[i+1,6] * 100)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check length of ma_infection_beds to make sure we got the expected number of values\n",
    "len(ma_infection_beds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the moving average to the moving average data frame ma_df\n",
    "ma_df['MA_Infection_Beds'] = ma_infection_beds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh7.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined DataFrames\n",
    "We will be combining the MA (moving average) dataframe with columns from the nh7 dataframe to make our final data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of columns you want to keep form nh7\n",
    "keep_cols = []\n",
    "for name in nh7.columns:\n",
    "    if name[0:4] not in ['Week', 'Resi', 'Staf', 'Numb']:\n",
    "        keep_cols.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if indices line up\n",
    "print('ma_df max indice: ', max(ma_df.index))\n",
    "print('nh7 max indice: ', max(nh7.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index to make sure the concatenation of the two df works as planned\n",
    "nh7.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(nh7.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh8 = pd.concat([ma_df, nh7[keep_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase Normality of Feature Distributions\n",
    "As you can see in the charts below, many of our independent variable are left or right skewed. PCA assumes that the features follow a Gaussian distribution, so to use this method we will need to apply some sort of transformation to the data. My first choice is a log transformation, but due to many of the numeric variables have 0's, we will use the cubed root transformation. Note above that went ahead and created our target and saved it to a separate variable. We will replace the transformed target variable later with the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh8.hist(figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cube root transformation is suggested with zero-inflated data b/c logistic is not feasible\n",
    "\n",
    "Features to give a cube root transformation to:\n",
    "- Staff_Weekly_Confirmed_COVID\n",
    "- Residents_Total_Confirmed_COVID_\n",
    "- Residents_Total_COVID_19_Deaths\n",
    "- Number_of_All_Beds\n",
    "- Staff_Total_Confirmed_Covid_19\n",
    "- Staff_Total_COVID_19_Deaths\n",
    "- Distinct_Beneficiaries\n",
    "- Episode_or_Stay_Count\n",
    "- Total_Charge_Amount\n",
    "- Percent_White_Beneficiaries \n",
    "- Percent_Black_Beneficiaries\n",
    "- Percent_Asian_Pacific_Islander_Beneficiaries\n",
    "- Percent_Hispanic_Beneficiaries (consider dropping that one outlier\n",
    "- Percent_American_Indian_or_Alaska_Native\n",
    "- Average_HCC_Score\n",
    "    \n",
    "I would suggest for an easy dimension reduction to have white and not-white instead of all the individual races. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through numeric columns and perform cube root transformation\n",
    "# rename new columns by adding prefix 'cbrt_'\n",
    "numerics = ['float64', 'int64']\n",
    "for c in nh8.columns:\n",
    "    if nh8[c].dtype in numerics:\n",
    "        new_col = 'cbrt_' + c\n",
    "        nh8[new_col] = np.cbrt(nh8[c])\n",
    "        nh8.drop([c], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh8.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh8.hist(figsize=(23,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Removal and Comparison\n",
    "We will examine plots of the data to check for outlies and deal with them as we see necessary. Remember that while some outliers will negatively affect the model, others add explainations to unexplored space in the model that could be valuable for prediction. Because we are looking at predicting by state, there aren't many data points so determining outliers would be challenging. Removing datapoints could also be detrimental to the model since there isn't much to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at how many data points per state\n",
    "for state in ['UT', 'NC', 'TX']:\n",
    "    print(state, 'datapoints: ', (nh8['State']==\"UT\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Cleaning\n",
    " - Get rid of the myweek variable\n",
    " - untransform (or use the original column) for MA_Infection_Beds because it will make it easier so we don't have to untransform the predictions\n",
    " - dummy variables for the categorical variables, but don't do leave one out for PCA because the rbgf doesn't require it, and neither does looking at random forest imporance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the cuberooted values with the original move value values for infection beds\n",
    "nh8['MA_Infection_Beds'] = ma_df['MA_Infection_Beds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns \n",
    "nh8.drop(['cbrt_MA_Infection_Beds', 'cbrt_myweek', 'cbrt_Distinct_Beneficiaries', 'cbrt_Percent_Male_Beneficiaries'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=nh8['cbrt_MA_Residents_Weekly_Confirmed_COVID'], y=nh8['MA_Infection_Beds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm values were dropped\n",
    "nh8.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into categorical and numeric\n",
    "cat_features = nh8.dtypes==object\n",
    "num_features = nh8.dtypes!=object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate data frames\n",
    "categorical_cols = nh8.columns[cat_features].to_list()\n",
    "numeric_cols = nh8.columns[num_features].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Pandas built in get_dummies for one-hot encoding\n",
    "dummies = pd.get_dummies(nh8[categorical_cols], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the dummy-variable df with the numeric df\n",
    "nh9 = pd.concat([nh8[numeric_cols],dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore missing values, check to see how many and whether appear missing at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize number of NA values in each column\n",
    "nh9.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NA Values\n",
    "nh10 = nh9.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is a df of the independent variables\n",
    "X = nh10.drop('MA_Infection_Beds', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is  a Pandas series of the target variable\n",
    "y = nh10['MA_Infection_Beds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick summary statistics of the target\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y)\n",
    "print(sum(y==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test sets where 70% of the data is for training \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression - Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running lasso regression, standardize design matrix (this is a model assumption).\n",
    "Use only the training set to fit scaler to prevent any leakage of information from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate model scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit scaler model to X_train\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_train using scaler model\n",
    "X_train_z = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_z = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the scaled data is now a np array and no longer a pd data frame\n",
    "type(X_test_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy variables do not need to be standardized so we are going to replace their standardized values with their original one-hot encoded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace transformed dummy variables with original values\n",
    "# get column indeces of dummy variables from nh10\n",
    "col1 = nh10.columns[nh10.dtypes == 'uint8'][0]\n",
    "print(col1)\n",
    "\n",
    "# -1 because argmax returns the index starting at 1 instead of 0\n",
    "first_dummy_var = np.argmax(nh10.columns == col1) - 1\n",
    "print(np.argmax(nh10.columns == col1))\n",
    "print(first_dummy_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the untransformed dummy variables from the original X_train and X_test\n",
    "X_train_dummies = X_train.iloc[:,first_dummy_var:]\n",
    "X_test_dummies = X_test.iloc[:,first_dummy_var:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make two df's to join\n",
    "std_values = pd.DataFrame(X_train_z, columns=X.columns).iloc[:,0:first_dummy_var]\n",
    "unstd_dummies = X_train.iloc[:,first_dummy_var:]\n",
    "\n",
    "# same for test data\n",
    "std_values_test = pd.DataFrame(X_test_z, columns=X.columns).iloc[:,0:first_dummy_var]\n",
    "unstd_dummies_test = X_test.iloc[:,first_dummy_var:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall that pulling from a subset df means we need ot reset the index before combining tables\n",
    "unstd_dummies.reset_index(inplace=True, drop=True)\n",
    "unstd_dummies_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two df's into one\n",
    "X_train_std = pd.concat([std_values, unstd_dummies], axis=1)\n",
    "X_test_std = pd.concat([std_values_test, unstd_dummies_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm shape is as expected\n",
    "X_train_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "\n",
    "# alpha = lambda for lasso regression \n",
    "# test 200 different alpha values to find optimal\n",
    "# cv = k-fold cross-validation, k=10\n",
    "nh_lassocv = LassoCV(alphas=None, n_alphas=200, cv=10, max_iter=1000000, random_state=76, fit_intercept=False, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit cross validation model to find best alpha (lambda)\n",
    "nh_lassocv.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lapha is very small indicating that our model will be very close to a regular linear model\n",
    "nh_lassocv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit our lasso regression model\n",
    "nh_lasso = Lasso(alpha=nh_lassocv.alpha_)\n",
    "nh_lasso.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'Coefficients': nh_lasso.coef_}, index=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.sort_values(by=['Coefficients'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the big advantages to using Lasso Regression for model selection is that it's regularization penalty pushes many of the regression coefficients to zero. This can help us see what variables we should consider dropping for future models. As you can see, many of the region locations and states have coefficients of 0 meaning they do not contribute to the model.="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame of coefficients that are between -0.01 and 0.01\n",
    "close_to_zero = (coef_df['Coefficients'] > -0.01) & (coef_df['Coefficients'] < 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.loc[close_to_zero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_mse = mean_squared_error(nh_lasso.predict(X_test_std), y_test)\n",
    "print(mean_squared_error(nh_lasso.predict(X_test_std), y_test))\n",
    "# note that because of the convergence issue I would not recommend using this model.\n",
    "# possible remedies to the convergence issue incude changering the tolerance, drastically increase iterations, or decreasing the number of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'Importance': rf.feature_importances_*100}, index=X_train.columns)\n",
    "importance = importance.iloc[rf.feature_importances_*100 > 1, :]\n",
    "#importance = importance.sort_values('Importance', axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable importance in random forests is not the same as regression coefficients. Variable importance is relative to all the other variables, and tells you how much information is gained if that variable is frequently chosen as the feature to split on across multiple trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance.plot(kind='barh', color='r')\n",
    "plt.xlabel('Variable Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(rf.predict(X_test), y_test))\n",
    "rf_mse = mean_squared_error(rf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.arange(1,25)\n",
    "pca_score = []\n",
    "\n",
    "for n in n_components:\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X_train_std)\n",
    "    X_pca = pca.transform(X_train_std)\n",
    "    pca_lm = LinearRegression()\n",
    "    score = cross_val_score(pca_lm, X_pca, y_train, scoring='neg_mean_squared_error').mean()\n",
    "    pca_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n_components[np.argmax(pca_score)]\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n)\n",
    "pca.fit(X_train_std)\n",
    "X_pca = pca.transform(X_train_std)\n",
    "pca_lm = LinearRegression()\n",
    "pca_lm.fit(X_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca = pca.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_lm_mse = mean_squared_error(y_test, pca_lm.predict(X_test_pca))\n",
    "print(mean_squared_error(y_test, pca_lm.predict(X_test_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.arange(1,25)\n",
    "pca_score = []\n",
    "\n",
    "for n in n_components:\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X_train_std)\n",
    "    X_pca = pca.transform(X_train_std)\n",
    "    pca_rf = RandomForestRegressor(random_state=76)\n",
    "    score = cross_val_score(pca_rf, X_pca, y_train, scoring='neg_mean_squared_error').mean()\n",
    "    pca_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n_components[np.argmax(pca_score)]\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n)\n",
    "pca.fit(X_train_std)\n",
    "X_pca = pca.transform(X_train_std)\n",
    "pca_rf = RandomForestRegressor(random_state=76)\n",
    "pca_rf.fit(X_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca = pca.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_rf_mse = mean_squared_error(y_test, pca_rf.predict(X_test_pca))\n",
    "print(mean_squared_error(y_test, pca_rf.predict(X_test_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost\n",
    "\n",
    "XGBoost is a specific Gradeint Boosting Machine, with optimization differences compared to traditional GBM's. XGBoost became popular after multiple Kaggle competitions were won using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatiate model for cross validation and hyperparameter selection\n",
    "boost_cv = XGBRegressor(random_state=76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of possibly hyperparameter values\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.2, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionar of the learning rates you want to try\n",
    "grid = dict(learning_rate = learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a grid search using 5 kfold cross validation measuring by negative mean squared error\n",
    "grid_search = GridSearchCV(boost_cv, grid, scoring=\"neg_mean_squared_error\", n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit grid result ot data\n",
    "grid_result = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at results\n",
    "grid_result.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find our learning rate based on nmse\n",
    "learning_rates[np.argmax(grid_result.cv_results_['mean_test_score'])-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = XGBRegressor(random_state=76, learning_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_lr2 = [mean_squared_error(boost.predict(X_test), y_test), 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbx_mse = mean_squared_error(boost.predict(X_test), y_test)\n",
    "print(gbx_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose champion model based on MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Lasso: \" + str(round(lasso_mse, 3)))\n",
    "print(\"Random Forest: \" + str(round(rf_mse, 3)))\n",
    "print(\"PCA LM: \" + str(round(pca_lm_mse, 3)))\n",
    "print(\"PCA RF: \" + str(round(pca_rf_mse, 3)))\n",
    "print(\"XGBoost: \" + str(round(gbx_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sasctl import Session, register_model, publish_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host=\"sasserver.demo.sas.com\"\n",
    "my_username='sasdemo'\n",
    "my_password='Orion123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a CAS session - there will be a warning because \n",
    "s = Session(hostname=host, username=my_username, password=my_password, verify_ssl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model and project name\n",
    "model_name = 'XGBoost Regressor NH3'\n",
    "project_name = 'Nursing Home Covid-19'\n",
    "\n",
    "# Register the model in SAS Model Manager\n",
    "model = register_model(boost, model_name, project_name, input=X_train, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export train and test data to CAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Connection to CAS using SWAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to CAS Server\n",
    "host=\"sasserver.demo.sas.com\"\n",
    "portnumber = 5570\n",
    "my_username='sasdemo'\n",
    "my_password='Orion123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=swat.CAS(host, portnumber, my_username, my_password)\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Connection\n",
    "sess.serverstatus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.table.caslibinfo(active=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view available caslibs and create NH caslib if not already done\n",
    "sess.table.caslibinfo()\n",
    "#sess.addcaslib(name='NH', path='/home/sasdemo/Python/SGF/', session=False, datasource={'srctype':'path'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change active caslib to Public\n",
    "sess.sessionProp.setSessOpt(caslib=\"NH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.table.caslibinfo(active=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test pandas df\n",
    "train = pd.concat([X_train, y_train], axis = 1)\n",
    "test = pd.concat([X_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pandas DataFrames to CAS\n",
    "sess.read_frame(train, casout={'caslib':'NH', 'name':'nh_train', 'promote':True})\n",
    "sess.read_frame(test, casout={'caslib':'NH', 'name':'nh_test', 'promote':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view that the tables were loaded into memory\n",
    "sess.table.tableinfo(caslib='NH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
